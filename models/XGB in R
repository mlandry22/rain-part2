# set up
setwd("D:/Kaggles/Rain")
library(data.table)
library(readr)
library(dplyr)
set.seed(222)


### SET VARIABLES AND FUNCTIONS ###

# columns to import
selection <- c(
  "Id", 
  "Expected",
  "minutes_past",             
  "radardist_km",                                                       
  "Ref", 
  "Ref_5x5_10th",
  "Ref_5x5_50th",
  "Ref_5x5_90th",
  "RefComposite", 
  "RefComposite_5x5_10th",
  "RefComposite_5x5_50th",
  "RefComposite_5x5_90th",
#   "RhoHV",
#   "RhoHV_5x5_10th",
#   "RhoHV_5x5_50th",
#   "RhoHV_5x5_90th",
#   "Zdr",
#   "Zdr_5x5_10th",
  "Zdr_5x5_50th",
# "Zdr_5x5_90th",
#   "Kdp",
#   "Kdp_5x5_10th",
  "Kdp_5x5_50th"
#   "Kdp_5x5_90th"  
)

# time spans between measurements for each Id
timeparse <- function(x){
  minutes <- x
  n <- length(minutes)
  timepassed <- vector(mode="numeric", length = n) 
  timepassed[1] <- minutes[1]
  timepassed[-1] <- diff(minutes)
  timepassed[n] <- 60-minutes[n-1]
  for (i in seq(2, n-1)) {                          # still some corner cases that may miscalculate
    if (minutes[i] < minutes[i-1]) {                # 34 IDs with single record plus other unique cases
      timepassed[i] <- minutes[i]  
    }
    if (minutes[i] > minutes[i+1]) {
      timepassed[i] <- 60-minutes[i-1]  
    }
  }
  return(timepassed/60)
}

# replacement of missing values                      
replacenas <- function(dt)  {                        
  norefidx <- which(is.na(dt[, Ref]))
  norefcidx <- which(is.na(dt[, RefComposite]))
  noref5idx <- which(is.na(dt[, Ref_5x5_50th]))
  
  subcidx <- setdiff(norefidx, norefcidx) 
  sub5idx <- setdiff(norefidx, noref5idx) 
  sub5idx <- setdiff(sub5idx, subcidx) 
  
  refc <- as.numeric(dt[, RefComposite])
  refc <- refc * 0.9201 + .1694
  dt[subcidx, "Ref"] <- refc[subcidx]
  
  ref5 <- as.numeric(dt[, Ref_5x5_50th])
  refc <- refc * 0.9605 + 1.3533
  dt[sub5idx, "Ref"] <- refc[sub5idx]
}

#estimation of  rainfall rates as a function of reflectivity
marshall_palmer <- function(dbz) ((10**(dbz/10))/200) ** (1/1.6)    # standard marshall-palmer function to find rainfall rate;
wsr_88d         <- function(dbz) ((10**(dbz/10))/300) ** (1/1.4)    # modified z-R function according to newer NWS standard;
nws_tropical    <- function(dbz) ((10**(dbz/10))/250) ** (1/1.2)    # profile for heavy rains and tropical storms
dorain <- function(dt) {                                             
  dt[, rate := wsr_88d(Ref)] 
  dt[, rateC := wsr_88d(RefComposite)]                              
}

# aggregate by Id and generate more features
collapsify <- function(dt) {
  dt[, .(
    target = log1p(mean(Expected, na.rm = T)),                          
    wref = mean(timespans * Ref, na.rm = T),         # look at slicing the timespans by midpoint instead of endpoint
    ref1 = mean(Ref_5x5_10th, na.rm = T),
    ref1sq = mean(Ref_5x5_10th^2, na.rm = T),        # any advantage to splitting the model in half after collapsing?  
    ref5 = mean(Ref_5x5_50th, na.rm = T),
    ref5sq = mean(Ref_5x5_50th^2, na.rm = T),
    ref9sq = mean(Ref_5x5_90th^2, na.rm = T),
    wrefc = mean(timespans * RefComposite, na.rm = T),
    refc1sq = mean(RefComposite_5x5_10th^2, na.rm = T),            
    refc9sq = mean(RefComposite_5x5_90th^2, na.rm = T),
    zdr5 = mean(Zdr_5x5_50th, na.rm = T),
    ratemax = max(rate, na.rm = T),         # max values cause warnings but will be purged with !is.na below
    refsd = sd(Ref, na.rm = T),
    ratesd = sd(rate, na.rm = T), 
    refcsd = sd(RefComposite, na.rm = T), 
    precip = sum(timespans * rate, na.rm = T),   
    precipC = sum(timespans * rateC, na.rm = T),
    refdiff = mean(Ref_5x5_50th-Ref, na.rm = T),
    refratio2 = mean((Ref_5x5_90th-Ref_5x5_10th)/Ref, na.rm = T), 
    refcratio2 = mean((RefComposite_5x5_90th-RefComposite_5x5_10th)/RefComposite, na.rm = T),
    rd = mean(radardist_km, na.rm = T),    
    rdxref = mean(radardist_km * Ref, na.rm = T),
    refdivrd = mean(Ref / radardist_km, na.rm = T),
    rdxrefc = mean(radardist_km * RefComposite, na.rm = T),
    refcdivrd = mean(RefComposite / radardist_km, na.rm = T),
    records = .N,
    refmissratio = sum(is.na(Ref))/.N
  ), Id]
}




### PROCESS DATA ####

# get train and do initial processing  
trraw <- fread("train.csv", select = selection)
trraw$timespans <- timeparse(trraw$minutes_past)
replacenas(trraw)
dorain(trraw)

# create a validation set                              
idnums <- unique(trraw$Id)
index <-sample(1:length(idnums), length(idnums)/5)                     
valraw <- trraw[Id %in% index, ]                                 
trraw <- trraw[!Id %in% index, ]  
valrawtemp <- valraw[, .(Id, minutes_past, Ref)]                        
write_csv(valrawtemp, "val_raw2.csv")
rm(valrawtemp)

#deal with outliers in training set
trraw <- subset(trraw, Expected <= 50)      
legit_reads <- 0.254 * 1:500                                      
trraw <- trraw[round(Expected, 4) %in% legit_reads]                 

# group by ID and clean up
train <- collapsify(trraw)
train <- train[!is.na(train$wref), ]                       # these types will not be scored in the test set, so dropping 
train <- as.data.frame(train) 
# fref2 <- lm(formula = train$ref ~ train$ref5)                            

#finish the validation set
val <- collapsify(valraw)
val <- val[!is.na(val$wref), ]                  
val <- as.data.frame(val)

# 
# keepers <- c("train", "val")                                 # remove this for full run
# rm(list= ls()[!(ls() %in% keepers)])
# save.image("modelinputs.RData")
# 
# 


# process test data
teraw <- fread("test.csv", select=selection)
teraw$timespans <- timeparse(teraw$minutes_past)
replacenas(teraw)
dorain(teraw)
teraw[, Expected := 0]      
test <- collapsify(teraw)
test[, target := NULL]
test <-as.data.frame(test) 

# clean up and save
keepers <- c("test", "train", "val")
rm(list= ls()[!(ls() %in% keepers)])
save.image("modelinputs.RData")


### BUILD MODEL ###


# prep for xgboost
library(xgboost)
library(Metrics)
set.seed(222)
bestround <- 150L

xgtrain <- xgb.DMatrix(as.matrix(train[, -c(1:2)]), label=train$target, missing=NA)

# set MAE eval function and model params 
meanabserr <- function(preds, xgtrain) {
  labels <- getinfo(xgtrain, "label")
  err <- mae(expm1(as.numeric(labels)), expm1(as.numeric(preds)))       
  return(list(metric = "MAE", value = err))
}  
param0 <- list(objective  = "reg:linear" 
  , eta = 0.5         
  , subsample = 1                               # model is not tuned
  , min_child_weight = 1
  , colsample_bytree = 0.7
  , max_depth = 5
)

# build model

# numrounds <- 150
# xgbfirst <- xgb.cv(data=xgtrain
#  , params=param0
#  , feval = meanabserr                           # could try RMSE as eval metric?
#  , nrounds=numrounds                                                         
#  , nfold=5
#  , verbose=1
#  , early.stop.round=NULL
# ) 
# plot(xgbfirst[, test.MAE.mean], xlim=c(0,numrounds), ylim=c(1.5,2.6))
# par(new=T)
# plot(xgbfirst[, train.MAE.mean], xlim=c(0,numrounds), ylim=c(1.5,2.6), xlab='', ylab='',axes=F)
# bestround <- which.min(xgbfirst[,test.MAE.mean]) 

watched <- list(eval=xgtrain)
xgbmod  <- xgb.train(data=xgtrain 
  , params=param0
  , feval = meanabserr
  , nrounds = bestround
  , verbose = 1
  , watchlist = watched
)
xgbfactors <- xgb.importance(colnames(train[-c(1:2)]), model=xgbmod)   
xgb.plot.importance(xgbfactors)



### VALIDATE ###


source("sample_solution2.R", echo = TRUE)

xgval <- xgb.DMatrix(as.matrix(val[, -c(1:2)]), missing=NA)
xgpred  <- predict(xgbmod, xgval)
validation <- data.frame(Id=val$Id, Predicted=expm1(xgpred), Truth=expm1(val$target))  # big piece of MAE is coming from true values > 4000
validation <- validation[order(validation$Predicted, validation$Truth), ]

# check for negative values and correct
zeroz <- vector(mode="numeric", length = nrow(validation))
validation$Predicted = pmax(zeroz, validation$Predicted)

# # fit the predictions to a distribution           
# library(fitdistrplus)                
# gfit <- fitdist(validation$Predicted, "gamma")
# denscomp(gfit)                  
# coef <- coef(gfit)
# gammavals <- rgamma(nrow(validation), coef[1], rate = coef[2])          # potential here for probability matching;
# gammavals <- sort(gammavals)                                            # could also try linear shifting and similar
# validation$Gammavals <- gammavals

# pull in the sample solution algorithm applied to the validation set
validation <- validation[order(validation$Id), ]
matchedrows <- match(validation$Id, results$Id)                         # this is to test blending with sample_solution.csv
validation$Sample <- results[matchedrows, Expected]    


#evaluate
mae(validation$Truth, validation$Predicted)
mae(validation$Truth, validation$Sample)        
write_csv(validation, "val-11-03-1.csv")                                # examine the file for sources of mae



### PREDICT ###

# process test data
xgtest = xgb.DMatrix(as.matrix(test[, -1]), missing = NA)

# predictions for each row of te
pr  <- predict(xgbmod, xgtest)  
xgb_prediction <- expm1(pr)   

# pull in sample values as a way to ensemble
# sample_sol <-fread("D:/Kaggles/Rain/sample_solution.csv")

# create the ensemble                      
res <- data.frame(Id=te$Id, Expected = 1 * xgb_prediction 
                  # + 0.0 * sample_sol$Expected
                  )  
# res$Predicted <- xgb_prediction      
# res$Sample <- sample_sol$Expected

write_csv(res, "xgb-11-03-1.csv")


